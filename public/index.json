[{"authors":"admin","categories":null,"content":"I am a PhD candidate in Finance, focusing on commodity markets through the lens of asset pricing and risk management. I hold a Master in Banking and Finance from the University of Paris Nanterre, and have five years of commodity market analysis experience, working for consulting and trading companies. I also have a strong interest in machine learning methods (applied to Finance, natural science and natural language processing) and algorithmic trading (high-frequency / latency). I am a French citizen living in Switzerland.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD candidate in Finance, focusing on commodity markets through the lens of asset pricing and risk management. I hold a Master in Banking and Finance from the University of Paris Nanterre, and have five years of commodity market analysis experience, working for consulting and trading companies. I also have a strong interest in machine learning methods (applied to Finance, natural science and natural language processing) and algorithmic trading (high-frequency / latency).","tags":null,"title":"Loïc Maréchal","type":"authors"},{"authors":null,"categories":["R"],"content":"\rIn this tutorial, I explain how to implement, in a flexible way, the algorithm of Bai, Lumsdaine, and Stock (1998).\nStep 1: Let’s begin by lagging our matrix of observations.\rThis function takes as argument a matrix of time series and lags it by an order (q).\nCode\rcompute_lags \u0026lt;- function(Y #time series matrix Y\r, q) #lag order q\r{\rp \u0026lt;- dim(Y)[1] #get the dimensions\rn \u0026lt;- dim(Y)[2]\rmyDates \u0026lt;- rownames(Y)[(q + 1) : p] #optional: keep the rownames dates of the data frame with final matching\rY \u0026lt;- data.matrix(Y) #matrix conversion\rYLAG \u0026lt;- matrix(data = NA, nrow \u0026lt;- (p - q), ncol \u0026lt;- (n * (q + 1))) #create an empty matrix\rfor(i in 0:q)\r{\rYLAG[ , (n * i + 1):(n * (i + 1))] \u0026lt;- Y[(q - i + 1):(p - i), ]\r}\rY \u0026lt;- YLAG[,1:n]\rYLAG \u0026lt;- YLAG[,(n + 1):dim(YLAG)[2]]\rreturn(list(Y = Y, YLAG = YLAG, myDates = myDates))\r}\r\r\r\rStep 2: Create all matrices\rThis function takes as argument, a matrix Y (of multivariate time series), an order q for the VAR, an optional matrix X of contemporaneous covariates,\rit adds an optional trend and determines whether we test for a break at the mean level (intercept = TRUE) or for all the parameters (intercept = FALSE).\nCode\rmatrix_conformation \u0026lt;- function(Y, q, X, trend, intercept)\r{\rpInit \u0026lt;- dim(Y)[1] #get the original number of observations\rlY \u0026lt;- compute_lags(Y, q) #get the list of contemporaneous and lags objects\rY \u0026lt;- lY$Y #get the matching dependent matrix\rYLAG \u0026lt;- lY$YLAG #get the lagged dependent variables matrix\rmyDates \u0026lt;- lY$myDates #get the original matching rowname vector (of dates)\rp \u0026lt;- dim(Y)[1] #length of the matrix\rnEq \u0026lt;- dim(Y)[2] #number of equations of the vAR\rprint(p)\rIn \u0026lt;- diag(nEq) #identity matrix of the number of equations of the VAR\rGt \u0026lt;- as.matrix(cbind(rep(1, p), YLAG)) #create a unique matrix transpose of G with one intercept and autoregressive terms\rn \u0026lt;- dim(Gt)[2] #incremental number of regressors\rif(!is.null(X)) #if additional covariates matrix is passed as argument\r{\rif(pInit==dim(X)[1]) #and if its size is equal to the original matrix Y\r{\rGt \u0026lt;- cbind(Gt, data.matrix(X[(q + 1) : pInit, ])) #increment Gt by the contemporaneous covariate matrix X\rn \u0026lt;- dim(Gt)[2] #increment the total number of regressors\r}\relse\rprint(\u0026quot;The number of observations of X does not match the one of Y\u0026quot;)\r}\rif(trend) #check if we add a trend\r{\rGt \u0026lt;- cbind(Gt, seq(1, p, by = 1)) #add trend\rn \u0026lt;- dim(Gt)[2] #increment the total number of regressors\r}\rif(intercept) #if only the intercept is allowed to break\r{\rs \u0026lt;- t(data.matrix(c(rep(0, n)))) #create the selection vector\rs[1] \u0026lt;- 1 #vector only select the first element (intercept) to test the shift\rS \u0026lt;- kronecker(s, In) #create the selection matrix\r}\rif(!intercept) #full parameters structural change estimation\r{ #get the full dimension of the test\rr \u0026lt;- nEq * n\rS \u0026lt;- diag(r) #identity matrix of the size of the test is the selection matrix\r}\rG \u0026lt;- t(Gt) #transpose Gt to get G as in BLS\rYex \u0026lt;- data.matrix(c(t(Y))) #Expend and vectorize Y\rGex \u0026lt;- kronecker(t(G), In) #Expend G\rreturn(list(Yex = Yex, Gex = Gex, p = p, G = G, S = S, myDates = myDates, Y = Y, nEq = nEq))\r}\r\r\rStep 3: We determine which is the optimal order of the VAR(q). For this we keep the option of using the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).\rWe add the argument qMax which determines up until which lag we are going to compute the criteria.\nCode\rcompute_aicbic \u0026lt;- function(Y, qMax, X, trend, intercept) #compute the AIC and BIC criteria for lags from 1 to 6\r{\rlibrary(stats) #load stats package\rAICBIC = matrix(data \u0026lt;- NA, nrow = 2, ncol = qMax) #create empty matrix for the AIC / BIC criteria\rfor(q in 1:qMax)\r{\rprint(paste0(\u0026quot;Testing lags number : \u0026quot;, q))\rlConfMatrix \u0026lt;- matrix_conformation(Y, q, X, trend, intercept) #create a list of conformed objects for the estimation\rYex \u0026lt;- lConfMatrix$Yex\rGex \u0026lt;- lConfMatrix$Gex\rmod \u0026lt;- lm(Yex~Gex) #estimate the model with lm\rAICBIC[1,q] \u0026lt;- AIC(mod) #get AIC\rAICBIC[2,q] \u0026lt;- BIC(mod) #get BIC\r}\rrownames(AICBIC) \u0026lt;- c(\u0026quot;AIC\u0026quot;, \u0026quot;BIC\u0026quot;)\rcolnames(AICBIC) \u0026lt;- paste0(\u0026quot;lags = \u0026quot;, 1:qMax)\rreturn(AICBIC)\r}\r\r\rStep 4: Let’s create a function to compute the parameters of the VAR(q).\rThis function is based on the pre-computed conformed matrix. We keep the option to compute it by Ordinary Least Squares (OLS), Feasible Generalized Least Squares (FGLS) or Iterative Generalized Least Squares (IGLS). FGLS and IGLS are required if we add additional covariates that are not common to all equations as in a Seemingly Unrelated Regression (SUR). The arguments are a matrix of covariates Z, which includes original and breaking covariates. Yex is an expended vector of the dependent variables Y, nEq is the number of equations, p is the number of observations, estMode is a string which can switch between “OLS”, “FGLS” and “IGLS”. In the case of “IGLS”, iter is an integer which determines the number of iterations used.\nCode\rcompute_beta \u0026lt;- function(Z, Yex, nEq, p, estMode, iter) #function which takes the regressor matrices Z and Yex, number of equations nEq\r#number of observations p, mode of estimation \u0026quot;estMode\u0026quot; and number of iterations\r#in the case of iterative feasible general least square\r{\rif(estMode == \u0026quot;OLS\u0026quot;) #if OLS\r{\rBeta \u0026lt;- solve(Z %*% t(Z), tol = 0) %*% Z %*% Yex #solve the system and get the vector of betas\r}\rif(estMode == \u0026quot;FGLS\u0026quot;) #if FGLS\r{\rBeta \u0026lt;- solve(Z %*% t(Z), tol = 0) %*% Z %*% Yex #solve the system and get the vector of betas\rSigma \u0026lt;- compute_sigma(Z, Yex, Beta, nEq) #get the covariance matrix of errors\rOmega \u0026lt;- kronecker(diag(p), Sigma) #get Omega\rBeta \u0026lt;- solve(Z %*% solve(Omega, tol = 0) %*% t(Z), tol = 0) %*% Z %*% solve(Omega, tol = 0) %*% Yex #solve the system and get the vector of betas\r}\rif(estMode == \u0026quot;IGLS\u0026quot;) #if IGLS\r{\rBeta \u0026lt;- solve(Z %*% t(Z), tol = 0) %*% Z %*% Yex #solve the system and get the vector of betas\rfor (i in 1:iter)\r{\rSigma \u0026lt;- compute_sigma(Z, Yex, Beta, nEq) #get the covariance matrix of errors\rOmega \u0026lt;- kronecker(diag(p), Sigma) #get Omega\rBeta \u0026lt;- solve(Z %*% solve(Omega, tol = 0) %*% t(Z), tol = 0) %*% Z %*% solve(Omega, tol = 0) %*% Yex #solve the system and get the vector of betas\r# print(Sigma) #control for convergence / non divergence\r}\r}\rSigma \u0026lt;- compute_sigma(Z, Yex, Beta, nEq) #final estimation of Sigma\rreturn(list(Beta = Beta, Sigma = Sigma))\r}\r\r\rStep 5: Let’s compute the covariance matrix of errors: Sigma.\rThis function simply compute the errors generated by the VAR in OLS, FGLS or IGLS mode.\nCode\rcompute_sigma \u0026lt;- function(Z, Yex, Beta, nEq) #compute the covariance matrix of errors as in BLS (1998)\r{\rerrors \u0026lt;- (Yex - t(Z) %*% Beta) #get the n*p vector of residuals\rErrors \u0026lt;- matrix(errors, ncol = nEq, byrow = T) #reset as matrix to obtain Sigma\rSigma \u0026lt;- cov(Errors)\rreturn(Sigma)\r}\r\r\rStep 6: We compute the F-statistic of the selected breaking parameters.\rCode\rcompute_fstat \u0026lt;- function(R, Beta, Z, p, Sigma) #compute the f-statistic as in BLS (1998)\r{\rRBeta \u0026lt;- R %*% Beta #pre compute the RBeta matrix with the selected coefficients allowed to break\rif(!is.null(Sigma)) #if the covariance matrix of error is passed as argument, compute F-stat\r{\rOmega \u0026lt;- kronecker(diag(p), Sigma) #get Omega\rFk \u0026lt;- p * t(RBeta) %*% solve(R %*% solve((Z %*% solve(Omega, tol = 0) %*% t(Z)) / p, tol = 0) %*% t(R), tol = 0) %*% RBeta\r}\rreturn(Fk)\r}\r\r\rStep 7: Before stepping into the confidence interval computation, let’s write a function computing the alphas based on critical values of the limiting distribution V (see, Picard, 1985).\rThis function takes a vector of critical values (e.g., 90%, 95% and 99%) and get the corresponding alpha levels.\nCode\rcompute_vdistr_cv = function(ci = c(0,9, 0.95, 0,99)) #computes the critical values for a vector of confidence intervals proposed, ci\r{\ru \u0026lt;- length(ci) #get the number of ci elements\rtarget \u0026lt;- 1 - (1 - ci) / 2 #redefine target for a two tail CI\rprint(paste0(\u0026quot;The vdistr targets are: \u0026quot;,target))\rx \u0026lt;- seq(-200, 200, 0.01) #define the support sequence \u0026quot;x\u0026quot; for the CDF of V\rV \u0026lt;- (3 / 2) * exp(abs(x)) * pnorm( (-3 / 2) * abs(x)^0.5 ) - (1 / 2) * pnorm( (-1 / 2) * abs(x)^0.5 ) #compute V\rcumV \u0026lt;- cumsum(V)/sum(V) #scale the CDF of V to reach one\r# dev.new() # plot(x, cumsum(gamma)/sum(gamma), t = \u0026#39;l\u0026#39;) #optionally plot V (nice shape!)\rcv \u0026lt;- rep(NA, u)\rk \u0026lt;- 1\rprint(target)\rfor(i in 2:length(x))\r{\rif(cumV[i-1] \u0026lt; target[k] \u0026amp;\u0026amp; cumV[i] \u0026gt;= target[k])\r{\rcv[k] \u0026lt;- x[i]\rk \u0026lt;- k + 1\rif(k\u0026gt;u)\rbreak\r} }\rprint(cv)\rreturn(cv)\r}\r\r\rStep 8: We compute the corresponding confidence intervals for a potential break date.\rThis function uses the original G matrix of covariates, the selection matrix S, the covariance matrix Sigma and the alphas from the critical values computed above.\nCode\rcompute_ci \u0026lt;- function(G, S, Sigma, R, Beta, cv, p) #as per Bekeart Harvey Lumsdaine (2002) // recall that RBeta = S %*% deltaT\r{\ru \u0026lt;- length(cv) #get the number of critical values from the vdistr\rciDelta \u0026lt;- rep(NA, u) #create empty vector\rRBeta \u0026lt;- R %*% Beta #pre compute the selection of (breaking) parameters to test\rtCi \u0026lt;- solve(t(RBeta) %*% S %*% kronecker((G %*% t(G)) / p, solve(Sigma, tol = 0)) %*% t(S) %*% RBeta, tol = 0) #compute the conf interval factor\rfor(i in 1:u)\r{\rciDelta[i] \u0026lt;- cv[i] * tCi #get the vector of critical values\r}\r# print(ciDelta)\rreturn(ciDelta)\r}\r\r\rStep 9: One step before launching the main function, get a nice graphical output for our results.\rCode\rcompute_plot_stats \u0026lt;- function(myDates, Variables, fstat, CI, Y, meanShift) #function to compute summary statistics and deliver plots\r{\rlibrary(ggplot2)\rlibrary(reshape2)\rlibrary(ggpubr)\rlibrary(stargazer)\rmyDates \u0026lt;- as.Date(myDates, format = \u0026quot;%d.%m.%Y\u0026quot;)\rmaxF \u0026lt;- which.max(fstat) #get the index when the break occurs\rcis \u0026lt;- round(CI[maxF, ]) #get the confidence intervals around the break\r#hard code for three different confidence intervals\rstartDate90 \u0026lt;- myDates[maxF - cis[1]] endDate90 \u0026lt;- myDates[maxF + cis[1]] startDate95 \u0026lt;- myDates[maxF - cis[2]] endDate95 \u0026lt;- myDates[maxF + cis[2]] startDate99 \u0026lt;- myDates[maxF - cis[3]] endDate99 \u0026lt;- myDates[maxF + cis[3]] p \u0026lt;- dim(Y)[1]\rn \u0026lt;- dim(Y)[2]\rY \u0026lt;- Y * 100 #get values in percentage\rY \u0026lt;- apply(Y, 2, as.double)\rY \u0026lt;- data.frame(cbind(myDates, Y))\rcolnames(Y)[1] \u0026lt;- \u0026quot;Date\u0026quot;\rcolnames(Y)[2:(n + 1)] \u0026lt;- Variables\rY[,1] \u0026lt;- myDates\rY \u0026lt;- melt(Y, id.var = \u0026quot;Date\u0026quot;) #reshape to long format\rnames(Y)[2] = \u0026quot;Variables\u0026quot;\rdev.new()\rg1 \u0026lt;- ggplot(Y, aes(x = Date, y = value, group = Variables, colour = Variables))\rg1 \u0026lt;- g1 + geom_line()\rg1 \u0026lt;- g1 + ggtitle(\u0026quot;\u0026quot;) + xlab(\u0026quot;Date\u0026quot;) + ylab(\u0026quot;Intensity (%)\u0026quot;)\rg1 \u0026lt;- g1 + coord_cartesian(ylim = c(0, max(Y$value, na.rm = T)))\rg1 \u0026lt;- g1 + scale_y_continuous(expand = c(0,0)) #force the y axis to start at zero\rg1 \u0026lt;- g1 + scale_x_date(breaks = scales::pretty_breaks(n = 10))\rg1 \u0026lt;- g1 + theme_bw()\r#add shaded area for various ci\rg1 \u0026lt;- g1+annotate(\u0026quot;rect\u0026quot;, xmin = startDate90, xmax = endDate90, ymin = 0, ymax = Inf,\ralpha = .6)\rg1 \u0026lt;- g1+annotate(\u0026quot;rect\u0026quot;, xmin = startDate95, xmax = endDate95, ymin = 0, ymax = Inf,\ralpha = .4)\rg1 \u0026lt;- g1+annotate(\u0026quot;rect\u0026quot;, xmin = startDate99, xmax = endDate99, ymin = 0, ymax = Inf,\ralpha = .2)\rd \u0026lt;- data.frame(date = myDates[maxF], event = \u0026quot;The event\u0026quot;)\rg1 \u0026lt;- g1 + geom_vline(data = d, mapping = aes(xintercept = date), color = \u0026quot;black\u0026quot;, size = 1) #add the break line\rreturn(g1)\r}\r\r\rStep 10: Finally, the main function, wrapping up all others and running a loop for all potential k breaks considered\rThis function takes all the aforementioned arguments, adding a trim parameter (in percent) to compute the burn-in and “burn-out” periods.\rIn addition, I add a boolean parameter (posBreak), which if set to TRUE will discard all breaks arising from a decrease in intercept or full parameters.\nCode\rmain \u0026lt;- function(Y #Y is a matrix or vector which will be lagged by (q) to compute a VAR(q)\r, X = NULL #X is a matrix of (contemporaneous) covariates\r, trend = FALSE #trend is a boolean indicating whether a trend vector should be added to the VAR\r, intercept = TRUE #intercept is a boolean indicating whether the test applies on the mean shift (TRUE) or all parameters (FALSE)\r, ci = c(0.9, 0.95, 0.99) #ci is the vector of confidence intervals (in growing order) to compute based on the CDF of a V distr.\r, estMode = \u0026quot;OLS\u0026quot; #estMode can take values of \u0026quot;OLS\u0026quot;, \u0026quot;FGLS\u0026quot;, \u0026quot;IGLS\u0026quot;\r, iter = 3 #in the case of \u0026quot;IGLS\u0026quot;, the number of iteration \u0026quot;iter\u0026quot; can be specified.\r, aicbicMode = \u0026quot;AIC\u0026quot; #AicbicMode can be \u0026quot;AIC\u0026quot; or \u0026quot;BIC\u0026quot; depending on the maximum criterion to select\r, qMax = 6 #qMax is the number of lags (from 1 to qMax) tested to determine the AIC / BIC maximum.\r, trim = 0.15 #trim is the percentage parameter to start and end the sample analysis\r, posBreak = FALSE #if we want the algorithm to only detect positive breaks\r)\r{\rmyVars \u0026lt;- colnames(Y) #get the variable names\rqOpt \u0026lt;- compute_aicbic(Y, qMax, X, trend, intercept) #return the AIC and BIC criteria for lags from 1 to 6\rprint(qOpt) #print the matrix of AIC and BIC for each lags\rq \u0026lt;- as.numeric(which.max(qOpt[aicbicMode,])) #choose the lag q according to the max AIC\rprint(paste0(\u0026quot;lag with the maximum \u0026quot;, aicbicMode, \u0026quot; is: \u0026quot;, q))\rlconf \u0026lt;- matrix_conformation(Y, q, X, trend, intercept) #create a list of conform objects for the estimation\rYex \u0026lt;- lconf$Yex #get the conformed (expanded) Yex matrix (for the system, in vector form)\rGex \u0026lt;- lconf$Gex #get the conformed G matrix of regressors for the system\rp \u0026lt;- lconf$p #final number of observations\rG \u0026lt;- lconf$G #original matrix of regressors\rS \u0026lt;- lconf$S #selection matrix\rY \u0026lt;- lconf$Y #matching original dependent variables matrix\rnEq \u0026lt;- lconf$nEq #original number of equations / dependent variables\rmyDates \u0026lt;- lconf$myDates #matching dates\rprint(paste0(\u0026quot;The number of equations in the system is: \u0026quot;, nEq))\rfstat \u0026lt;- rep(NA, p) #create a vector of f_statistics for each k tested\rmeanShift \u0026lt;- rep(NA, p) #create a vector with the evaluated size of the intercept difference\rCI \u0026lt;- matrix(data = NA, nrow = p, ncol = length(ci)) #create a matrix of confidence intervals for each k tested\rcv \u0026lt;- compute_vdistr_cv(ci) #compute critical values for the vector of confidence intervals proposed\rstartInd \u0026lt;- round(trim * p) #start index\rendInd \u0026lt;- round(p - trim * p) #end index\rfor(k in startInd:endInd) #loop over the k with a trimming date / burn period\r{\rif(k%%10==0)\rprint(paste0(\u0026quot;The iteration is at the level k = \u0026quot;, k)) #get an idea of where we are in the loop every 10 iterations\rGexB \u0026lt;- Gex %*% t(S)\rGexB[1:((k - 1) * (nEq)),] \u0026lt;- 0 #force filling the GexB matrix with 0 before and original values after k\rZ \u0026lt;- t(cbind(Gex, GexB)) #bind the regressor and breaking regressor matrices together\rlbetaSigma \u0026lt;- compute_beta(Z, Yex, nEq, p, estMode, iter) #compute the BetaSigma object list\rBeta \u0026lt;- lbetaSigma$Beta #get the vector of betas\rSigma \u0026lt;- lbetaSigma$Sigma #get the covariance matrix of errors\rpBeta \u0026lt;- length(Beta) #get the length of the vector of betas\r#create a selection matrix to get only the betas of interest (breakings)\rif(intercept) #1 - case where only shift in intercept\r{\rR \u0026lt;- matrix(data = 0, nrow = nEq, ncol = pBeta)\rR[,(pBeta - nEq + 1):pBeta] \u0026lt;- diag(nEq)\r}\rif(!intercept) #2 - case where all parameters break\r{\rR \u0026lt;- matrix(data = 0, nrow = pBeta / 2 , ncol = pBeta)\rR[,(pBeta / 2 + 1):pBeta] \u0026lt;- diag(pBeta / 2)\r}\rfstat[k] \u0026lt;- compute_fstat(R, Beta, Z, p, Sigma) #compute the F-statistic for the current k\rCI[k, ] \u0026lt;- compute_ci(G, S, Sigma, R, Beta, cv, p) #compute the confidence interval for the current k\rmeanShift[k] \u0026lt;- mean(R %*% Beta) #get the mean intercept shift\r}\rif(posBreak) #if posBreak is TRUE, limit to positive break detection\rfstat[meanShift \u0026lt; 0] \u0026lt;- 0\rdev.new() plot(fstat) #plots the generated sequence of F-statistics\rg1 \u0026lt;- compute_plot_stats(myDates, myVars, fstat, CI, Y)\rbreakInd \u0026lt;- which.max(fstat)\rbreakDate \u0026lt;- myDates[breakInd]\rbreakCi \u0026lt;- CI[breakInd, ]\rrownames(Y) \u0026lt;- myDates\rGt \u0026lt;- t(G)\rrownames(Gt) \u0026lt;- myDates\rmeanShift \u0026lt;- meanShift[breakInd]\rmaxF = max(fstat, na.rm=T)\rtrimDates = matrix(data=c(myDates[startInd], myDates[endInd]),nrow = 1, ncol = 2)\rcolnames(trimDates) \u0026lt;- c(\u0026quot;begin trim date\u0026quot;, \u0026quot;end trim date\u0026quot;)\rreturn(list(fstat = fstat #return a \u0026quot;multibreak\u0026quot; class object\r, maxF = maxF\r, confInterval = breakCi\r, criticalValues = cv\r, breakDate = breakDate\r, Y = data.frame(Y)\r, G = data.frame(Gt)\r, breakInd = breakInd\r, meanShift = meanShift\r, aicbic = qOpt\r, g1 = g1\r, trimDates = trimDates))\r}\r\r\rSimulation: now that everything is ready, let’s generate some clean data to test whether we are able to identify break dates.\rThis extra function generates a matrix of p time series of n observations, based on the R rnorm generation. The argument intensity is a double value which is added to the generated data after the break that we can set to occur at any percent of the dataset (here at 35%)\nCode\rcompute_simul \u0026lt;- function(n, p, intensity = 1, whenbreak = 0.35)\r{\rset.seed(123) #optional\rprebreakN \u0026lt;- round(n * whenbreak)\rpostbreakN \u0026lt;- round(n * (1 - whenbreak))\rY \u0026lt;- rbind(matrix(rnorm(prebreakN * p) + 5, nrow = prebreakN, ncol = p), matrix(rnorm(postbreakN * p) + 5 + intensity, nrow = postbreakN, ncol = p))\rstartDate \u0026lt;- as.Date(\u0026quot;22.01.2020\u0026quot;, format = \u0026quot;%d.%m.%Y\u0026quot;) #date when I wrote this code\rsimulDates \u0026lt;- 0:(n - 1) + startDate\rsimulDates \u0026lt;- format(simulDates, format = \u0026quot;%d.%m.%Y\u0026quot;)\rrownames(Y) \u0026lt;- simulDates\rcolnames(Y) \u0026lt;- LETTERS[1:p]\rprint(head(Y))\rreturn(Y)\r}\r\r\rGraphical result with three simulated variables\rFig 1. Break detection for three simulated variables experiencing a common break at 35% of the sample, with an intensity parameter of four. The order of the VAR(q) is 10, after selection by the BIC, with no trend or additional covariates added. The break is easy to identify visually and is well captured by the algorithm. Max F-statistic: 159.34, and the confidence intervals for 90%, 95% and 99%, represented by the shaded areas are very concentrated around the break.\n\r\rGraphical result with five simulated variables\rFig 2. Break detection for five simulated variables experiencing a common break at 35% of the sample, with an intensity parameter of 0.5. The order of the VAR(q) is 10, after selection by the BIC, with no trend or additional covariates added. The break is impossible to detect visually. However, the algorithm accurately captures it. Max F-statistic: 22.72 and the confidence intervals for 90%, 95% and 99%, represented by the shaded areas are much wider.\n\rTo get the full working package, including data, you can clone or import the project from my github repository\n\r","date":1579745594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579745594,"objectID":"a449276ad68f5c1f3ac18b56e3d4876e","permalink":"/post/multivariate_break_test/","publishdate":"2020-01-22T21:13:14-05:00","relpermalink":"/post/multivariate_break_test/","section":"post","summary":"In this tutorial, I explain how to implement, in a flexible way, the algorithm of Bai, Lumsdaine, and Stock (1998).\nStep 1: Let’s begin by lagging our matrix of observations.\rThis function takes as argument a matrix of time series and lags it by an order (q).\nCode\rcompute_lags \u0026lt;- function(Y #time series matrix Y\r, q) #lag order q\r{\rp \u0026lt;- dim(Y)[1] #get the dimensions\rn \u0026lt;- dim(Y)[2]\rmyDates \u0026lt;- rownames(Y)[(q + 1) : p] #optional: keep the rownames dates of the data frame with final matching\rY \u0026lt;- data.","tags":["R Markdown","plot","structural breaks","multivariate time series","matrix computation"],"title":"Multivariate break test","type":"post"},{"authors":null,"categories":["Java"],"content":"\rIn this tutorial, I explain how to emulate navigation on the SEC website, perform bulk downloads of forms (e.g. 10-K forms) and extract for each company a sentiment grade, based on basic natural language processing methods (dictionaries). The full Java package including two dictionaries of positive and negative tone words is available on my github. The SEC used to provide all forms on a FTP server, however, two years ago they stopped it and we now need this little workaround to perform bulk downloads. This tutorial is inspired from a class given to the students in the Msc in Finance at my University (Neuchâtel).\nStep 1: download index files (*.idx) from the SEC website.\rCode\rpackage edgar;\rimport java.io.File;\rimport java.io.IOException;\rimport java.net.MalformedURLException;\rimport java.net.URL;\rimport java.nio.file.Files;\rimport java.nio.file.Paths;\rimport java.util.Scanner;\rimport org.apache.commons.io.FileUtils;\rpublic class GetFiles {\rprivate static int beginYear = 1999;\rprivate static int endYear = 2016;\rprivate static int beginQtr = 1;\rprivate static int endQtr = 4;\rprotected static String myBaseDestination = \u0026quot;10K\\\\\u0026quot;;\rpublic static void main(String[] args) throws IOException{\rrunUrls(beginYear, endYear, beginQtr, endQtr);\r}\rpublic static void runUrls(int beginYear, int endYear, int beginQtr, int endQtr) throws IOException{\rString myBeginString = \u0026quot;idxFiles\\\\\u0026quot;;\rString myEndingString = \u0026quot;.company.idx\u0026quot;;\rString myStringFile = \u0026quot;\u0026quot;;\rFile myDestinationFile;\rfor(int i = beginYear; i \u0026lt;= endYear; i++)\r{ for(int j = beginQtr; j \u0026lt;= endQtr;j ++)\r{\rmyStringFile = myBeginString+String.valueOf(i)+\u0026quot;QTR\u0026quot;+String.valueOf(j)+myEndingString;\r//System.out.println(myStringFile);\rString content = new String(Files.readAllBytes(Paths.get(myStringFile)));\r//parse and download the matching files\rparseContent(content, i);\r}\r}\r}\rpublic static void parseContent(String content, int year)\r{\r// first get read of the first lines.);\rScanner contentScanner = new Scanner(content);\rint lineCount = 0;\rwhile(contentScanner.hasNextLine()) {\rString next = contentScanner.nextLine();\r//ignore the first 9 lines\rif(lineCount \u0026gt; 9){\rSystem.out.println(next);\r//get the cik\rString cik = next.substring(74,86).replaceAll(\u0026quot;\\\\s\u0026quot;,\u0026quot;\u0026quot;); //get the form type\rString formType = next.substring(62,74).replaceAll(\u0026quot;\\\\s\u0026quot;,\u0026quot;\u0026quot;);;\r//System.out.println(formType);\rif(formType.equals(\u0026quot;10-K\u0026quot;)){\rString urlToDownload = next.substring(98,150).replaceAll(\u0026quot;\\\\s\u0026quot;,\u0026quot;\u0026quot;);\rSystem.out.println(urlToDownload);\rdownloadFiles(\u0026quot;https://www.sec.gov/Archives/\u0026quot;+urlToDownload, new File(myBaseDestination+String.valueOf(year)+\u0026quot;/\u0026quot;+urlToDownload.replaceAll(\u0026quot;/\u0026quot;, \u0026quot;.\u0026quot;)));\r//alternative, use the function already defined in the G_getIdx class:\r//G_getIdx.downloadIdx(\u0026quot;https://www.sec.gov/Archives/\u0026quot;+urlToDownload, new File(myBaseDestination+String.valueOf(year)+\u0026quot;/\u0026quot;+urlToDownload.replaceAll(\u0026quot;/\u0026quot;, \u0026quot;.\u0026quot;)));\r}\r}\rlineCount++;\r}\r}\rpublic static void downloadFiles(String urlToDownload, File destinationFile){\rURL url = null;\rtry {\rurl = new URL(urlToDownload);\r} catch (MalformedURLException e) {\re.printStackTrace();\r}\rtry {\rFileUtils.copyURLToFile(url, destinationFile);\r} catch (IOException e) {\re.printStackTrace();\r}\r}\r}\r\r\rStep 2: Use the downloaded index files (*.idx) to recognize the type of forms wanted (here 10-K)\rCode\rpackage edgar;\rimport org.apache.commons.io.*;\rimport java.io.File;\rimport java.io.IOException;\rimport java.net.MalformedURLException;\rimport java.net.URL;\rpublic class GetIdx {\r//define the first year of the index files\rprivate static int beginYear = 1999;\r//define the last year of the index files\rprivate static int endYear = 2018;\r//same for quarters\rprivate static int beginQtr = 1;\rprivate static int endQtr = 4;\r//program entry point\rpublic static void main(String[] args){\rrunUrls(beginYear, endYear, beginQtr, endQtr);\r}\r//core method accessing every quarter year index file\rpublic static void runUrls(int beginYear, int endYear, int beginQtr, int endQtr){\rString myBeginString = \u0026quot;https://www.sec.gov/Archives/edgar/full-index/\u0026quot;;\rString myEndingString = \u0026quot;/company.idx\u0026quot;;\rString myStringUrl = \u0026quot;\u0026quot;;\rString myBaseDestination = \u0026quot;idxFiles\\\\\u0026quot;;\rFile myDestinationFile;\rfor(int i = beginYear; i \u0026lt;= endYear; i++){\rfor(int j = beginQtr; j \u0026lt;= endQtr;j ++){\rmyStringUrl = myBeginString+String.valueOf(i)+\u0026quot;/QTR\u0026quot;+String.valueOf(j)+myEndingString;\rSystem.out.println(myStringUrl);\rmyDestinationFile = new File(myBaseDestination+String.valueOf(i)+\u0026quot;QTR\u0026quot;+String.valueOf(j)+\u0026quot;.company.idx\u0026quot;);\rSystem.out.println(\u0026quot;downloading: \u0026quot;+String.valueOf(i)+\u0026quot;QTR\u0026quot;+String.valueOf(j)+\u0026quot;.company.idx\u0026quot;);\rdownloadIdx(myStringUrl, myDestinationFile); }\r}\r}\r//method called to download the files\rpublic static void downloadIdx(String baseUrl, File destinationFile){\rURL urlUrl = null;\rtry {\rurlUrl = new URL(baseUrl);\r} catch (MalformedURLException e) {\re.printStackTrace();\r}\rtry {\rFileUtils.copyURLToFile(urlUrl, destinationFile);\r} catch (IOException e) {\re.printStackTrace();\r}\r}\r}\r\r\rStep 3: Parse the downloaded forms to retrieve their date of submission, company identifiers and names and compute a basic sentiment grade\rCode\rpackage edgar;\rimport java.io.*;\rimport java.util.ArrayList;\rimport java.util.List;\rimport java.util.regex.*;\rimport org.apache.commons.io.*;\rpublic class ParseFiles {\r// 10K All Years, a folder with all downloaded files sorted to parse.\rprivate static final String sortedPath = \u0026quot;10K\\\\1999\\\\\u0026quot;;\rprivate static final String csvOutputPath = \u0026quot;outputSentiment.csv\u0026quot;;\rprivate static final File negativeDictionary = new File(\u0026quot;dictionary\\\\negativeWords.txt\u0026quot;);\rprivate static final File positiveDictionary = new File(\u0026quot;dictionary\\\\positiveWords.txt\u0026quot;);\r//private static final File positiveDictionary = new File(\u0026quot;C:\\\\Users\\\\aerial\\\\javaWorkspace\\\\dictionary\\\\positiveWords.txt\u0026quot;);\rprivate static String myOutputContent;\rprivate static List\u0026lt;String\u0026gt; positiveWords = new ArrayList\u0026lt;String\u0026gt;();\rprivate static List\u0026lt;String\u0026gt; negativeWords = new ArrayList\u0026lt;String\u0026gt;();\rprivate static String[] nameArray;\rprivate static String[] dateArray;\rprivate static String[] cikArray;\r//two arrays for the positive and negative score\rprivate static double[] positiveScoreArray;\rprivate static double[] negativeScoreArray;\rpublic static void main(String[] args) throws IOException { //load the positive and negative words array lists with the dictionaryReader method\rnegativeWords = dictionaryReader(negativeDictionary);\rpositiveWords = dictionaryReader(positiveDictionary);\r//load all files from the path\rFile listOfFiles[] = fileLoader(new File(sortedPath));\rSystem.out.println(listOfFiles);\rnameArray = new String[listOfFiles.length];\rdateArray = new String[listOfFiles.length];\rcikArray = new String[listOfFiles.length];\rpositiveScoreArray = new double[listOfFiles.length];\rnegativeScoreArray = new double[listOfFiles.length];\rfor (int i = 0; i \u0026lt; listOfFiles.length; i++){\rSystem.out.println(\u0026quot;index parsed is: \u0026quot;+i+\u0026quot; out of: \u0026quot;+String.valueOf(listOfFiles.length));\rFile file = listOfFiles[i];\rSystem.out.println(file);\rmyOutputContent = fileReader(file);\rSystem.out.println(\u0026quot;fileReader is done\u0026quot;);\rfileParser(myOutputContent, i);\rSystem.out.println(\u0026quot;fileParser is done\u0026quot;);\rgetSentiment(myOutputContent, i);\rSystem.out.println(\u0026quot;getSentiment is done\u0026quot;);\r}\rinfoWriter(nameArray, dateArray, cikArray, negativeScoreArray, positiveScoreArray);\r}\r//read all the files of the folders / subfolders. and return them.\rpublic static File[] fileLoader(File folderPath){\rFile[] listOfFiles = folderPath.listFiles();\rreturn (listOfFiles);\r} //returns the final complete text file in the form of a single string.\rpublic static String fileReader(File file) throws IOException{\rString myOutputContent = null;\rif (file.isFile() \u0026amp;\u0026amp; file.getName().endsWith(\u0026quot;.txt\u0026quot;)){\rString myCharSet = null;\rmyOutputContent = FileUtils.readFileToString(file, myCharSet);\r}\rreturn(myOutputContent);\r}\r//parse the content of the file string and isolate company names and cik using the regex abilities of Java.\rpublic static void fileParser(String fileContent, int index) throws IOException{\rPattern myBeginPattern = Pattern.compile(\u0026quot;COMPANY\\\\s*CONFORMED\\\\s*NAME\\\\s*:\\\\s*\u0026quot;);\rPattern myEndPattern = Pattern.compile(\u0026quot;CENTRAL\\\\s*INDEX\\\\s*KEY\\\\s*:\\\\s*\u0026quot;);\rMatcher myBeginMatcher = myBeginPattern.matcher(fileContent);\rMatcher myEndMatcher = myEndPattern.matcher(fileContent); int beginChar = (myBeginMatcher.find() ? myBeginMatcher.end() : -1);\rint endChar = (myEndMatcher.find() ? myEndMatcher.start() : -1);\rString myCompanyName = fileContent.substring(beginChar,endChar);\rmyCompanyName = myCompanyName.trim().replaceAll(\u0026quot;\\n\u0026quot;, \u0026quot;\u0026quot;);\rSystem.out.println(myCompanyName);\rmyBeginPattern = Pattern.compile(\u0026quot;CONFORMED\\\\s*PERIOD\\\\s*OF\\\\s*REPORT\\\\s*:\\\\s*\u0026quot;);\rmyEndPattern = Pattern.compile(\u0026quot;FILED\\\\s*AS\\\\s*OF\\\\s*DATE\\\\s*:\\\\s*\u0026quot;);\rmyBeginMatcher = myBeginPattern.matcher(fileContent);\rmyEndMatcher = myEndPattern.matcher(fileContent); beginChar = (myBeginMatcher.find() ? myBeginMatcher.end() : -1);\rendChar = (myEndMatcher.find() ? myEndMatcher.start() : -1);\rString myFiscalEndYear = fileContent.substring(beginChar,endChar);\rmyFiscalEndYear = myFiscalEndYear.trim().replaceAll(\u0026quot;\\n\u0026quot;, \u0026quot;\u0026quot;);\rSystem.out.println(myFiscalEndYear);\rmyBeginPattern = Pattern.compile(\u0026quot;CENTRAL\\\\s*INDEX\\\\s*KEY\\\\s*:\\\\s*\u0026quot;);\rmyEndPattern = Pattern.compile(\u0026quot;STANDARD\\\\s*INDUSTRIAL\\\\s*CLASSIFICATION\\\\s*:\\\\s*\u0026quot;);\rmyBeginMatcher = myBeginPattern.matcher(fileContent);\rmyEndMatcher = myEndPattern.matcher(fileContent); beginChar = (myBeginMatcher.find() ? myBeginMatcher.end() : -1);\rendChar = (myEndMatcher.find() ? myEndMatcher.start() : -1);\rString cik = fileContent.substring(beginChar,endChar);\rcik = cik.trim().replaceAll(\u0026quot;\\n\u0026quot;, \u0026quot;\u0026quot;);\rSystem.out.println(cik);\rnameArray[index] = myCompanyName;\rdateArray[index] = myFiscalEndYear;\rcikArray[index] = cik;\r}\rprivate static void getSentiment(String textFile, int indexFile) throws IOException{\rfor (int i = 0; i \u0026lt; positiveWords.size(); i++)\r{\rif (textFile.toLowerCase().contains(positiveWords.get(i).toLowerCase()))\rpositiveScoreArray[indexFile]++;\r}\rfor (int i = 0; i \u0026lt; negativeWords.size(); i++)\r{\rif (textFile.toLowerCase().contains(negativeWords.get(i).toLowerCase()))\rnegativeScoreArray[indexFile]++;\r} }\rpublic static List\u0026lt;String\u0026gt; dictionaryReader(File dictionary) throws IOException{\rBufferedReader br = new BufferedReader(new FileReader(dictionary));\rString line;\rList\u0026lt;String\u0026gt; wordList = new ArrayList\u0026lt;String\u0026gt;();\rwhile ((line = br.readLine()) != null){\rwordList.add(line);\rSystem.out.println(line);\r}\rreturn wordList;\r}\rpublic static void infoWriter(String[] nameArray, String[] dateArray, String[] cikArray, double[] negativeScoreArray, double[] positiveScoreArray) throws IOException{\rFileWriter writer = new FileWriter(csvOutputPath,false);\rwriter.append(\u0026quot;company name\u0026quot;);\rwriter.append(\u0026#39;,\u0026#39;);\rwriter.append(\u0026quot;date\u0026quot;);\rwriter.append(\u0026#39;,\u0026#39;);\rwriter.append(\u0026quot;cik\u0026quot;);\rwriter.append(\u0026#39;,\u0026#39;);\rwriter.append(\u0026quot;negative score\u0026quot;);\rwriter.append(\u0026#39;,\u0026#39;);\rwriter.append(\u0026quot;positive score\u0026quot;);\rwriter.append(\u0026#39;,\u0026#39;);\rwriter.append(\u0026quot;normalized overall score\u0026quot;);\rwriter.append(\u0026#39;\\n\u0026#39;);\rfor(int i = 0; i\u0026lt;nameArray.length; i++)\r{\rwriter.append(nameArray[i]);\rwriter.append(\u0026#39;,\u0026#39;);\rwriter.append(dateArray[i]);\rwriter.append(\u0026#39;,\u0026#39;);\rwriter.append(cikArray[i]);\rwriter.append(\u0026#39;,\u0026#39;);\rwriter.append(String.valueOf(negativeScoreArray[i]));\rwriter.append(\u0026#39;,\u0026#39;);\rwriter.append(String.valueOf(positiveScoreArray[i]));\rwriter.append(\u0026#39;,\u0026#39;);\rwriter.append(String.valueOf((positiveScoreArray[i]-negativeScoreArray[i])/(positiveScoreArray[i]+negativeScoreArray[i])));\rwriter.append(\u0026#39;\\n\u0026#39;);\r}\rwriter.flush();\rwriter.close();\r}\r}\rTo get the full working package (including dictionaries), you can clone or import the project from my github repository\n\r\r","date":1579745594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579745594,"objectID":"fb148f01f1ddfdf8b5372d6cb156639a","permalink":"/post/edgar-web-browsing-emulation-sentiment/","publishdate":"2020-01-22T21:13:14-05:00","relpermalink":"/post/edgar-web-browsing-emulation-sentiment/","section":"post","summary":"In this tutorial, I explain how to emulate navigation on the SEC website, perform bulk downloads of forms (e.g. 10-K forms) and extract for each company a sentiment grade, based on basic natural language processing methods (dictionaries). The full Java package including two dictionaries of positive and negative tone words is available on my github. The SEC used to provide all forms on a FTP server, however, two years ago they stopped it and we now need this little workaround to perform bulk downloads.","tags":["Java","SEC","NLP","web scrapping","sentiment extraction"],"title":"Security Exchange Commission: web browsing emulation and natural language processing","type":"post"},{"authors":[],"categories":null,"content":"","date":1579132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579132800,"objectID":"47776f792b3d17fe6a60b1079da31594","permalink":"/talk/2020hfr_poster/","publishdate":"2019-08-28T00:00:00Z","relpermalink":"/talk/2020hfr_poster/","section":"talk","summary":"Talk given during the 2019 Non Standard Investment Choice Workshop","tags":[],"title":"Dauphine 2020 Hedge Fund Conference - poster","type":"talk"},{"authors":null,"categories":["R"],"content":"\rEfficient frontier\rIn this post, I quickly describe how to compute and update an efficient frontier in adding stocks to an existing porfolio with R.\nFirst let’s write a simple code for an efficient frontier computation\n\refficient_frontier = function(MRet #matrix of returns (MRet)\r, rangeMu) #range (sequence) of target expected returns (rangeMu)\r{\ruM \u0026lt;- dim(MRet)[1] #get the row (uM) and column (pM) dimensions of the matrix of returns\rpM \u0026lt;- dim(MRet)[2]\rexpRet \u0026lt;- colMeans(MRet) #compute the portfolio\u0026#39;s individual stocks expected returns\rOmega = var(MRet) #compute the sample var-covar matrix\runityVec \u0026lt;- rep(1, pM) #define a constraints vector (weights of the portfolio must sum to one)\rA \u0026lt;- rbind(expRet, unityVec) #define a matrix of constraints (weights sum to one and variance will match a #target level of expected returns) n \u0026lt;- length(rangeMu) #get the length of the target range (sequence)\rmyVar \u0026lt;- rep(NA, n) #define an empty variance vector\rmyWeights \u0026lt;- matrix(data = NA, nrow = n, ncol = pM) #define an empty matrix of weights for each stock at each level of target\r#expected returns\rfor(i in 1:n) #loop over the target expected returns range and compute variances and weights { b \u0026lt;- matrix(data = c(rangeMu[i], 1), nrow = 2)\rmyVar[i] \u0026lt;- t(b) %*% solve(A %*% solve(Omega) %*% t(A)) %*% b\rmyWeights[i,] \u0026lt;- solve(Omega) %*% t(A) %*% solve(A %*% solve(Omega) %*% t(A)) %*% b\r}\rmySd \u0026lt;- myVar^0.5 #compute the standard deviation vector\rreturn(mySd) #return the standard deviation vector\r}\r\rIncremental efficient frontier\r\refficient_frontier_increment = function(MRet) #define a function that takes as argument a matrix of returns (MRet)\r{\rpM \u0026lt;- dim(MRet)[2] #get the column (pM) dimension of the matrix of returns\rrangeMu \u0026lt;- seq(-0.01, 0.05, 0.001) #hard code the vector of expected returns (can be passed as argument)\rmySd \u0026lt;- efficient_frontier(MRet[, 1:9], rangeMu) #compute the sd vector of the portfolio composed of the first nine stocks dev.new() #get a new plot\rplot(mySd * 100 #plot the efficient frontier of the portfolio composed of the first 9 stocks\r, rangeMu * 100\r, xlab = \u0026quot;volatility (%)\u0026quot;\r, ylab = \u0026quot;expected return (%)\u0026quot;\r, t = \u0026#39;l\u0026#39;\r, xlim = c(0, max(mySd * 100))\r, ylim = c(-1, 5)) Sys.sleep(2) #enjoy the chart for two seconds\rfor(j in 10:pM) #loop over all stocks and increment the portfolio with the next stock\r{\rmySd \u0026lt;- efficient_frontier(MRet[, 1:j], rangeMu) #recompute the efficient frontier\rlines(mySd * 100, rangeMu * 100, col = \u0026#39;red\u0026#39;) #add a line of the more efficient frontier on the chart\rSys.sleep(0.1) #define how long you want to wait before the next one\r}\r}\r\rResult in animation\rFig 1. Animation of an efficient frontier with stock increment\n\rTo run the code with real historical figures, I propose a *.csv file of NYSE returns\rDownload nyse_stocks_historical_returns.csv\n\r","date":1577758394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577758394,"objectID":"72513296b7bd1daded4aab7d8aef537d","permalink":"/post/incremental_efficient_frontier/","publishdate":"2019-12-30T21:13:14-05:00","relpermalink":"/post/incremental_efficient_frontier/","section":"post","summary":"Efficient frontier\rIn this post, I quickly describe how to compute and update an efficient frontier in adding stocks to an existing porfolio with R.\nFirst let’s write a simple code for an efficient frontier computation\n\refficient_frontier = function(MRet #matrix of returns (MRet)\r, rangeMu) #range (sequence) of target expected returns (rangeMu)\r{\ruM \u0026lt;- dim(MRet)[1] #get the row (uM) and column (pM) dimensions of the matrix of returns\rpM \u0026lt;- dim(MRet)[2]\rexpRet \u0026lt;- colMeans(MRet) #compute the portfolio\u0026#39;s individual stocks expected returns\rOmega = var(MRet) #compute the sample var-covar matrix\runityVec \u0026lt;- rep(1, pM) #define a constraints vector (weights of the portfolio must sum to one)\rA \u0026lt;- rbind(expRet, unityVec) #define a matrix of constraints (weights sum to one and variance will match a #target level of expected returns) n \u0026lt;- length(rangeMu) #get the length of the target range (sequence)\rmyVar \u0026lt;- rep(NA, n) #define an empty variance vector\rmyWeights \u0026lt;- matrix(data = NA, nrow = n, ncol = pM) #define an empty matrix of weights for each stock at each level of target\r#expected returns\rfor(i in 1:n) #loop over the target expected returns range and compute variances and weights { b \u0026lt;- matrix(data = c(rangeMu[i], 1), nrow = 2)\rmyVar[i] \u0026lt;- t(b) %*% solve(A %*% solve(Omega) %*% t(A)) %*% b\rmyWeights[i,] \u0026lt;- solve(Omega) %*% t(A) %*% solve(A %*% solve(Omega) %*% t(A)) %*% b\r}\rmySd \u0026lt;- myVar^0.","tags":["R Markdown","plot","efficient frontier","optimal portfolio choice","matrix computation"],"title":"Incremental efficient frontier","type":"post"},{"authors":[],"categories":null,"content":"","date":1576627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576627200,"objectID":"25101ce0c3dbc9d315d4e31b2b221780","permalink":"/talk/2019nzfm_discussion/","publishdate":"2019-08-28T00:00:00Z","relpermalink":"/talk/2019nzfm_discussion/","section":"talk","summary":"Talk given during the 2019 New Zealand Finance Meeting","tags":[],"title":"New Zealand Finance Meeting 2019 - discussion","type":"talk"},{"authors":[],"categories":null,"content":"","date":1576627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576627200,"objectID":"8643b7e2557b454abbeb42a551097f40","permalink":"/talk/2019nzfm_presentation/","publishdate":"2019-08-28T00:00:00Z","relpermalink":"/talk/2019nzfm_presentation/","section":"talk","summary":"Talk given during the 2019 New Zealand Finance Meeting","tags":[],"title":"New Zealand Finance Meeting 2019 - presentation","type":"talk"},{"authors":[],"categories":null,"content":"","date":1575590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575590400,"objectID":"1b3555f4b7da3dcb49502f498ee5aecf","permalink":"/talk/2019sfm_presentation/","publishdate":"2019-08-28T00:00:00Z","relpermalink":"/talk/2019sfm_presentation/","section":"talk","summary":"Talk given during the 2019 Securities and Financial Markets Conference","tags":[],"title":"Securities and Financial Markets Conference 2019 - presentation","type":"talk"},{"authors":null,"categories":null,"content":"This research combines recent advances in the Realized Volatility (RV) literature and three specific commodity futures factors to improve the forecasts of commodity volatility. The three forecasting variables are the term structure slope, the time to maturity and a measure of supply and demand uncertainty. I first assess these variables’ empirical contribution to commodity futures volatility, in adding them in RV forecast models. First in the univariate HAR-RV of Corsi (2009) and second in the multivariate VAR-RV of Andersen, Bollerslev, Diebold, and Labys (2003). The long-term memory of assets RV justifies the former, whereas the \u0026ldquo;financialization\u0026rdquo; of commodities and the resulting commodity connectedness, supports the latter. I evaluate the out of sample validity of these forecast models and propose one risk management application. Hence, this research is important for both economic understanding and risk management purposes.\n","date":1569542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569542400,"objectID":"a27da18f061082c3b212f77268362146","permalink":"/project/internal-project2/","publishdate":"2019-09-27T00:00:00Z","relpermalink":"/project/internal-project2/","section":"project","summary":"This research combines recent advances in the Realized Volatility (RV) literature and three specific commodity futures factors to improve the forecasts of commodity volatility. The three forecasting variables are the term structure slope, the time to maturity and a measure of supply and demand uncertainty. I first assess these variables’ empirical contribution to commodity futures volatility, in adding them in RV forecast models. First in the univariate HAR-RV of Corsi (2009) and second in the multivariate VAR-RV of Andersen, Bollerslev, Diebold, and Labys (2003).","tags":["realized volatility"],"title":"A comprehensive look at commodity volatility forecasting","type":"project"},{"authors":null,"categories":null,"content":"We identify and date a significant surge in the amount of investment tracking commodity futures indices, a phenomenon identified heretofore with anecdotal or visual evidences. Using a difference-in-differences setting on cumulative abnormal log price changes, computed with several benchmarks during the roll window of the SP-GSCI, we first find that the uncovered break in the speculative investment structure had an alleviating effect. Second, we explain the abnormal nearby and first deferred contracts price changes by measures of risk (liquidity) premium required at long (short) term horizon by speculative (hedging) activity. Finally, in a cruder market efficiency framework, we find that transaction costs incurred by an arbitrager (price taker) explain most of the abnormal term-structure change with a coefficient close to unity. In addition, this abnormal change -which is of 17 basis points at most- is never significant once we adjust the standard errors for event-induced variance and cross-correlation.\n","date":1569542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569542400,"objectID":"5adc183a3a61c682fe6937d3aa682296","permalink":"/project/internal-project1/","publishdate":"2019-09-27T00:00:00Z","relpermalink":"/project/internal-project1/","section":"project","summary":"We identify and date a significant surge in the amount of investment tracking commodity futures indices, a phenomenon identified heretofore with anecdotal or visual evidences. Using a difference-in-differences setting on cumulative abnormal log price changes, computed with several benchmarks during the roll window of the SP-GSCI, we first find that the uncovered break in the speculative investment structure had an alleviating effect. Second, we explain the abnormal nearby and first deferred contracts price changes by measures of risk (liquidity) premium required at long (short) term horizon by speculative (hedging) activity.","tags":["commodity index investment"],"title":"The valuation effects of index investment in commodity futures","type":"project"},{"authors":[],"categories":null,"content":"","date":1568764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568764800,"objectID":"29e2290e673a5258ffec41a29d3ad706","permalink":"/talk/2019nsic_presentation/","publishdate":"2019-08-28T00:00:00Z","relpermalink":"/talk/2019nsic_presentation/","section":"talk","summary":"Talk given during the 2019 Non Standard Investment Choice Workshop","tags":[],"title":"ESSEC 2019 Non Standard Investment Choice Workshop - presentation","type":"talk"},{"authors":[],"categories":null,"content":"","date":1565222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565222400,"objectID":"435f1e37a1ca2b04c522a86065308e18","permalink":"/talk/2019dmc_discussion/","publishdate":"2019-08-28T00:00:00Z","relpermalink":"/talk/2019dmc_discussion/","section":"talk","summary":"Discussion given during the 2019 Queenstown Derivatives Markets Conference","tags":[],"title":"Queenstown 2019 Derivatives Markets Conference - discussion","type":"talk"},{"authors":[],"categories":null,"content":"","date":1565222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565222400,"objectID":"1aa5ac288dc2c314b8782029834c75d6","permalink":"/talk/2019dmc_presentation/","publishdate":"2019-08-28T00:00:00Z","relpermalink":"/talk/2019dmc_presentation/","section":"talk","summary":"Talk given during the 2019 Queenstown Derivatives Markets Conference","tags":[],"title":"Queenstown 2019 Derivatives Markets Conference - presentation","type":"talk"},{"authors":[],"categories":null,"content":"","date":1559779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559779200,"objectID":"1e8f82a480734d8e5d7d0a8b46e93b67","permalink":"/talk/2019iaf_presentation/","publishdate":"2019-08-28T00:00:00Z","relpermalink":"/talk/2019iaf_presentation/","section":"talk","summary":"Talk given during 2019 IAF Research Day","tags":[],"title":"IAF 2019 Research Day - presentation","type":"talk"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let's make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"}]